{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQGEIbgAoDMENAIUhuAGgMAQ3ABTm/wG9LhQd7bwFWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x197537b9780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9909792196\n"
     ]
    }
   ],
   "source": [
    "def cost(theta0, theta1, x, y):\n",
    "    h = theta0 + theta1 * x\n",
    "    predictions = x.dot(theta1)\n",
    "    sqerrors = (predictions - y)**2\n",
    "    result = 1/(2 * x.size) * sqerrors.sum()\n",
    "    return result\n",
    "\n",
    "\n",
    "print(cost(2, 0.5, x, y))\n",
    "#(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    #your code here\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
